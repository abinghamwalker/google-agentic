{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW8T9bn5Qx41"
      },
      "outputs": [],
      "source": [
        "# Google ADK Code example of parallelization\n",
        "\n",
        "# Part of agent.py --> Follow https://google.github.io/adk-docs/get-started/quickstart/ to learn the setup\n",
        " # --- 1. Define Researcher Sub-Agents (to run in parallel) ---\n",
        "\n",
        " # Researcher 1: Renewable Energy\n",
        " researcher_agent_1 = LlmAgent(\n",
        "     name=\"RenewableEnergyResearcher\",\n",
        "     model=GEMINI_MODEL,\n",
        "     instruction=\"\"\"You are an AI Research Assistant specializing in energy.\n",
        " Research the latest advancements in 'renewable energy sources'.\n",
        " Use the Google Search tool provided.\n",
        " Summarize your key findings concisely (1-2 sentences).\n",
        " Output *only* the summary.\n",
        " \"\"\",\n",
        "     description=\"Researches renewable energy sources.\",\n",
        "     tools=[google_search],\n",
        "     # Store result in state for the merger agent\n",
        "     output_key=\"renewable_energy_result\"\n",
        " )\n",
        "\n",
        " # Researcher 2: Electric Vehicles\n",
        " researcher_agent_2 = LlmAgent(\n",
        "     name=\"EVResearcher\",\n",
        "     model=GEMINI_MODEL,\n",
        "     instruction=\"\"\"You are an AI Research Assistant specializing in transportation.\n",
        " Research the latest developments in 'electric vehicle technology'.\n",
        " Use the Google Search tool provided.\n",
        " Summarize your key findings concisely (1-2 sentences).\n",
        " Output *only* the summary.\n",
        " \"\"\",\n",
        "     description=\"Researches electric vehicle technology.\",\n",
        "     tools=[google_search],\n",
        "     # Store result in state for the merger agent\n",
        "     output_key=\"ev_technology_result\"\n",
        " )\n",
        "\n",
        " # Researcher 3: Carbon Capture\n",
        " researcher_agent_3 = LlmAgent(\n",
        "     name=\"CarbonCaptureResearcher\",\n",
        "     model=GEMINI_MODEL,\n",
        "     instruction=\"\"\"You are an AI Research Assistant specializing in climate solutions.\n",
        " Research the current state of 'carbon capture methods'.\n",
        " Use the Google Search tool provided.\n",
        " Summarize your key findings concisely (1-2 sentences).\n",
        " Output *only* the summary.\n",
        " \"\"\",\n",
        "     description=\"Researches carbon capture methods.\",\n",
        "     tools=[google_search],\n",
        "     # Store result in state for the merger agent\n",
        "     output_key=\"carbon_capture_result\"\n",
        " )\n",
        "\n",
        " # --- 2. Create the ParallelAgent (Runs researchers concurrently) ---\n",
        " # This agent orchestrates the concurrent execution of the researchers.\n",
        " # It finishes once all researchers have completed and stored their results in state.\n",
        " parallel_research_agent = ParallelAgent(\n",
        "     name=\"ParallelWebResearchAgent\",\n",
        "     sub_agents=[researcher_agent_1, researcher_agent_2, researcher_agent_3],\n",
        "     description=\"Runs multiple research agents in parallel to gather information.\"\n",
        " )\n",
        "\n",
        " # --- 3. Define the Merger Agent (Runs *after* the parallel agents) ---\n",
        " # This agent takes the results stored in the session state by the parallel agents\n",
        " # and synthesizes them into a single, structured response with attributions.\n",
        " merger_agent = LlmAgent(\n",
        "     name=\"SynthesisAgent\",\n",
        "     model=GEMINI_MODEL,  # Or potentially a more powerful model if needed for synthesis\n",
        "     instruction=\"\"\"You are an AI Assistant responsible for combining research findings into a structured report.\n",
        "\n",
        " Your primary task is to synthesize the following research summaries, clearly attributing findings to their source areas. Structure your response using headings for each topic. Ensure the report is coherent and integrates the key points smoothly.\n",
        "\n",
        " **Crucially: Your entire response MUST be grounded *exclusively* on the information provided in the 'Input Summaries' below. Do NOT add any external knowledge, facts, or details not present in these specific summaries.**\n",
        "\n",
        " **Input Summaries:**\n",
        "\n",
        " *   **Renewable Energy:**\n",
        "     {renewable_energy_result}\n",
        "\n",
        " *   **Electric Vehicles:**\n",
        "     {ev_technology_result}\n",
        "\n",
        " *   **Carbon Capture:**\n",
        "     {carbon_capture_result}\n",
        "\n",
        " **Output Format:**\n",
        "\n",
        " ## Summary of Recent Sustainable Technology Advancements\n",
        "\n",
        " ### Renewable Energy Findings\n",
        " (Based on RenewableEnergyResearcher's findings)\n",
        " [Synthesize and elaborate *only* on the renewable energy input summary provided above.]\n",
        "\n",
        " ### Electric Vehicle Findings\n",
        " (Based on EVResearcher's findings)\n",
        " [Synthesize and elaborate *only* on the EV input summary provided above.]\n",
        "\n",
        " ### Carbon Capture Findings\n",
        " (Based on CarbonCaptureResearcher's findings)\n",
        " [Synthesize and elaborate *only* on the carbon capture input summary provided above.]\n",
        "\n",
        " ### Overall Conclusion\n",
        " [Provide a brief (1-2 sentence) concluding statement that connects *only* the findings presented above.]\n",
        "\n",
        " Output *only* the structured report following this format. Do not include introductory or concluding phrases outside this structure, and strictly adhere to using only the provided input summary content.\n",
        " \"\"\",\n",
        "     description=\"Combines research findings from parallel agents into a structured, cited report, strictly grounded on provided inputs.\",\n",
        "     # No tools needed for merging\n",
        "     # No output_key needed here, as its direct response is the final output of the sequence\n",
        " )\n",
        "\n",
        "\n",
        " # --- 4. Create the SequentialAgent (Orchestrates the overall flow) ---\n",
        " # This is the main agent that will be run. It first executes the ParallelAgent\n",
        " # to populate the state, and then executes the MergerAgent to produce the final output.\n",
        " sequential_pipeline_agent = SequentialAgent(\n",
        "     name=\"ResearchAndSynthesisPipeline\",\n",
        "     # Run parallel research first, then merge\n",
        "     sub_agents=[parallel_research_agent, merger_agent],\n",
        "     description=\"Coordinates parallel research and synthesizes the results.\"\n",
        " )\n",
        "\n",
        " root_agent = sequential_pipeline_agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langchain example of parralelization\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "from typing import Optional\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import Runnable, RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure your API key environment variable is set (e.g., OPENAI_API_KEY)\n",
        "try:\n",
        "    llm: Optional[ChatOpenAI] = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "    if llm:\n",
        "        print(f\"Language model initialized: {llm.model_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing language model: {e}\")\n",
        "    llm = None\n",
        "\n",
        "\n",
        "# --- Define Independent Chains ---\n",
        "# These three chains represent distinct tasks that can be executed in parallel.\n",
        "\n",
        "summarize_chain: Runnable = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Summarize the following topic concisely:\"),\n",
        "        (\"user\", \"{topic}\")\n",
        "    ])\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "questions_chain: Runnable = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Generate three interesting questions about the following topic:\"),\n",
        "        (\"user\", \"{topic}\")\n",
        "    ])\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "terms_chain: Runnable = (\n",
        "    ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Identify 5-10 key terms from the following topic, separated by commas:\"),\n",
        "        (\"user\", \"{topic}\")\n",
        "    ])\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "# --- Build the Parallel + Synthesis Chain ---\n",
        "\n",
        "# 1. Define the block of tasks to run in parallel. The results of these,\n",
        "#    along with the original topic, will be fed into the next step.\n",
        "map_chain = RunnableParallel(\n",
        "    {\n",
        "        \"summary\": summarize_chain,\n",
        "        \"questions\": questions_chain,\n",
        "        \"key_terms\": terms_chain,\n",
        "        \"topic\": RunnablePassthrough(),  # Pass the original topic through\n",
        "    }\n",
        ")\n",
        "\n",
        "# 2. Define the final synthesis prompt which will combine the parallel results.\n",
        "synthesis_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"Based on the following information:\n",
        "     Summary: {summary}\n",
        "     Related Questions: {questions}\n",
        "     Key Terms: {key_terms}\n",
        "     Synthesize a comprehensive answer.\"\"\"),\n",
        "    (\"user\", \"Original topic: {topic}\")\n",
        "])\n",
        "\n",
        "# 3. Construct the full chain by piping the parallel results directly\n",
        "#    into the synthesis prompt, followed by the LLM and output parser.\n",
        "full_parallel_chain = map_chain | synthesis_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# --- Run the Chain ---\n",
        "async def run_parallel_example(topic: str) -> None:\n",
        "    \"\"\"\n",
        "    Asynchronously invokes the parallel processing chain with a specific topic\n",
        "    and prints the synthesized result.\n",
        "\n",
        "    Args:\n",
        "        topic: The input topic to be processed by the LangChain chains.\n",
        "    \"\"\"\n",
        "    if not llm:\n",
        "        print(\"LLM not initialized. Cannot run example.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Running Parallel LangChain Example for Topic: '{topic}' ---\")\n",
        "    try:\n",
        "        # The input to `ainvoke` is the single 'topic' string, which is\n",
        "        # then passed to each runnable in the `map_chain`.\n",
        "        response = await full_parallel_chain.ainvoke(topic)\n",
        "        print(\"\\n--- Final Response ---\")\n",
        "        print(response)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during chain execution: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_topic = \"The history of space exploration\"\n",
        "    # In Python 3.7+, asyncio.run is the standard way to run an async function.\n",
        "    asyncio.run(run_parallel_example(test_topic))"
      ],
      "metadata": {
        "id": "3rYaMP20J4Si"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}